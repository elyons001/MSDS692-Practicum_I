{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ea9f02-efa4-4693-89ed-44a299c4d46a",
   "metadata": {},
   "source": [
    "# 01 DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e8b13-850c-4086-b22c-87a47ffe1f11",
   "metadata": {},
   "source": [
    "# <center>Evaluation of Gradient Boosted Models for Hard Drive Failure Prediction in Data Centers</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af968a3-d9dd-4d53-bc56-985b891f9833",
   "metadata": {},
   "source": [
    "In recent years, the expansion of the internet of things (IoT) and industrial internet of things (IIoT) connectivity and sensors have contributed to advancements in predictive maintenance (Market Research Future, 2022). In fact, by 2030 the predictive maintenance market is forecasted to reach $111.34 billion USD (Market Research Future, 2022). Historically, maintenance primarily consisted of corrective and preventative (Maintenance (technical), 2022). Of the two maintenance types, corrective maintenance is often more expensive due to consequential part damage or downtime (Maintenance (technical), 2022). Preventative maintenance can also be costly due to the unwarranted replacement of parts (Maintenance (technical), 2022). In contrast, predictive maintenance utilizes sensors to monitor equipment health conditions, which is combined with analytics to predict and prevent unexpected equipment failures (Predictive maintenance, 2022). Predictive maintenance offers benefits such as cost savings, improved reliability, and reduced downtime (Predictive maintenance, 2022).  \n",
    "<br>\n",
    "One industry that can and is benefitting from predictive maintenance are data centers. Depending on the type of the data center, various levels of fault tolerances and redundancy exist, which equate to a range of uptime requirements from 99.671% (28.8 hours annual downtime) to 99.995% (26.3 minutes annual downtime) (Hewlett Packard Enterprise Development LP, 2022). To maintain the high level of uptime and availability, predictive maintenance is being leveraged to forecast failure on equipment including, but not limited to, generators, power distribution units (PDU), transfer switches, transformers, and uninterruptible power supplies (UPS) (Tyrrell, 2022). Not only is data center ancillary equipment benefitting from predictive maintenance, but data center computing equipment such as hard drives might also benefit.  \n",
    "<br>\n",
    "Accurately predicting hard drive failures within a data center can ensure operational readiness, improve reliability, and reduce costs. Although there are protective measures in place to distribute files or objects over different hard drives and locations, hard drive failures still present a risk of data loss to customers (Wilson, 2018).  \n",
    "<br>\n",
    "The project will determine if predictive maintenance using machine learning can be leveraged to proactively identify and replace failing hard drives to mitigate these risks. Furthermore, the project will evaluate three gradient boosted classifier models including histogram-based gradient boosting, CatBoost, and XGBoost to determine which model provides the best evaluation metrics, specifically F-scores (F1 and F2). The F1 score is the harmonic mean between precision and recall (F-score, 2022). The F2 score values recall more, by applying additional weights (F-score, 2022).    \n",
    "<br>\n",
    "To attempt to answer the question, hard drive data from within a data center enivornment must be collected for analysis. Backblaze is a cloud company that provides data storage strategies. In 2015, Backblaze started publishing daily hard drive snapshots for each hard drive in Backblaze Data Centers (Beach, 2015). The daily hard drive snapshots provide specific information for each hard drive, such as date, serial number, model, capacity (bytes), failure (0 - operational, 1 - failed), and S.M.A.R.T. attributes (normalized and raw) in a comma separated value (CSV) file (Beach, 2015). The S.M.A.R.T. (Self-Monitoring, Analysis and Reporting Technology) or SMART attributes provide health indicators and statistics for the hard drive (S.M.A.R.T., 2022). These data fields will be explored in greater depth throughout the project. Between 2013 and 2015, the daily hard drive snaphosts were published in a yearly ZIP file consisting of 365 CSV files. In 2016, the daily hard drive snapshots were published in quarterly ZIP files consisting of 90-92 CSV files depending on quarter and year. Backblaze makes the ZIP files available for download through HTTPS download URLs.  \n",
    "<br>\n",
    "The project will focus on the Backblaze hard drive data for the first quarter of 2022.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8b4e0-64a9-4036-8750-ab84ea57c55c",
   "metadata": {},
   "source": [
    "## Overview of the Jupyter Notebook  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32914d8e-5190-45f3-ab43-e744c4f64567",
   "metadata": {},
   "source": [
    "The notebook intends to create a data directory and using functions, the hard drive snapshots for the calendar quarter are downloaded from the Backblaze website in a ZIP file. The hard drive snapshots are stored in CSV files and the CSV files are extracted from the quarterly ZIP file. The CSV files are moved to a corresponding directory for the quarter within the data directory. Lastly, a Parquet file is created and examined for the quarter of CSV files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250960ea-7b20-461f-b40e-e8a0b96226de",
   "metadata": {},
   "source": [
    "## Import modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac06663-d3bc-4bcc-8633-f47c3fb857b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:18:17.925287Z",
     "iopub.status.busy": "2022-10-22T02:18:17.924864Z",
     "iopub.status.idle": "2022-10-22T02:18:18.302075Z",
     "shell.execute_reply": "2022-10-22T02:18:18.301082Z",
     "shell.execute_reply.started": "2022-10-22T02:18:17.925233Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f5735-c79f-43bd-84e4-4763c222de84",
   "metadata": {},
   "source": [
    "## Create data directory  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e129c-c9c6-4bda-a1b2-f63e93e61ea5",
   "metadata": {},
   "source": [
    "To maintain file organization and structure, a `data` directory is created (if not already existent) and the `data` path is mapped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb5c9de-531a-44e9-972f-3562b1be700c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:18:18.303960Z",
     "iopub.status.busy": "2022-10-22T02:18:18.303623Z",
     "iopub.status.idle": "2022-10-22T02:18:18.307920Z",
     "shell.execute_reply": "2022-10-22T02:18:18.307356Z",
     "shell.execute_reply.started": "2022-10-22T02:18:18.303938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# # https://docs.python.org/3/library/pathlib.html\n",
    "# https://docs.python.org/3/reference/compound_stmts.html\n",
    "# https://docs.python.org/3/library/exceptions.html\n",
    "# https://docs.python.org/3/tutorial/errors.html\n",
    "# https://docs.python.org/3/library/os.html\n",
    "\n",
    "# The current working directory is assigned to \n",
    "# the 'cwd_path' variable.\n",
    "cwd_path = Path.cwd()\n",
    "\n",
    "# Try to make the 'data' directory, but if the directory \n",
    "# already exists then raise and print and excpetion.\n",
    "try:\n",
    "    os.makedirs(cwd_path.joinpath('data'), exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print(\"Directory already exists\")\n",
    "\n",
    "# The 'data' directory is joined to the 'cwd_path' and assigned \n",
    "# to the 'data_path' variable.\n",
    "data_path = cwd_path.joinpath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4967984-ae4b-4f43-aeac-291fee46ad05",
   "metadata": {},
   "source": [
    "## Download the quarterly ZIP file from Backblaze website  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae4502-edec-46f5-8c4d-4c6acafe915e",
   "metadata": {},
   "source": [
    "A function is defined and used to download the ZIP file for the first quarter of 2022 from the Backblaze website.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855c6415-fa4b-4051-bcae-f89e91cdfdd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:18:18.308735Z",
     "iopub.status.busy": "2022-10-22T02:18:18.308558Z",
     "iopub.status.idle": "2022-10-22T02:18:18.313937Z",
     "shell.execute_reply": "2022-10-22T02:18:18.313429Z",
     "shell.execute_reply.started": "2022-10-22T02:18:18.308718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# References:\n",
    "# https://requests.readthedocs.io/en/latest/\n",
    "# https://docs.python.org/3/library/shutil.html\n",
    "\n",
    "def download_file(url, file):\n",
    "    \"\"\"\n",
    "    Download a file from a website using the URL path\n",
    "    and file name. Move the downloaded file to the to the \n",
    "    '/data' directory of the current working \n",
    "    directory.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- the URL path location of the file to be downloaded\n",
    "    file -- the file name to be download\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using a concatenation of the URL path ('url') \n",
    "    # and file name ('file), the 'url_file' variable is created.\n",
    "    url_file = url + file\n",
    "    \n",
    "    # A HTTP request to get the file ('url_file') is made \n",
    "    # and a Response object ('r') is received.\n",
    "    # If necessary, redirects are allowed.\n",
    "    r = requests.get(url_file, allow_redirects=True)\n",
    "    \n",
    "    # Using the 'with' statemeent, the file to be downloaded ('file')\n",
    "    # is opened with the writing binary ('wb') mode.\n",
    "    # The content from the HTTP request response object ('r') is \n",
    "    # written to the opened file ('fd').\n",
    "    # The downloaded file ('file') is moved from the current working \n",
    "    # directory to the 'data_path' directory.\n",
    "    # the 'data_path' directorty, the file is overwritten. \n",
    "    # In the event, there is an exception, an error is raised\n",
    "    # and printed.\n",
    "    with open(file, 'wb') as fd:\n",
    "        fd.write(r.content)\n",
    "        src = str(file)\n",
    "        dst = str(data_path) + '/' + str(file)\n",
    "        try:\n",
    "            shutil.move(src, dst)\n",
    "            print(f\"File downloaded: {file}\")\n",
    "        except BaseException as err:\n",
    "            print(f\"Error: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5183bce-13c6-42f4-aeb5-2717ab19c94d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:18:18.314698Z",
     "iopub.status.busy": "2022-10-22T02:18:18.314523Z",
     "iopub.status.idle": "2022-10-22T02:18:18.319339Z",
     "shell.execute_reply": "2022-10-22T02:18:18.318914Z",
     "shell.execute_reply.started": "2022-10-22T02:18:18.314681Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign the 'dl_file' variable to the zip file ('data_Q1_2022.zip') to \n",
    "# be downloaded.\n",
    "dl_file = 'data_Q1_2022.zip'\n",
    "\n",
    "# Assign the 'url' variable to the URL path where the zip files \n",
    "# are located on the website.\n",
    "url = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a2fbbe-4f69-4c2f-9b27-ec7929ca1474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:18:18.320148Z",
     "iopub.status.busy": "2022-10-22T02:18:18.319925Z",
     "iopub.status.idle": "2022-10-22T02:19:24.087091Z",
     "shell.execute_reply": "2022-10-22T02:19:24.086567Z",
     "shell.execute_reply.started": "2022-10-22T02:18:18.320130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: data_Q1_2022.zip\n"
     ]
    }
   ],
   "source": [
    "# The URL location ('url') and the file ('df_file') to be downloaded \n",
    "# are passed to the 'download_file' function.\n",
    "download_file(url, dl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8e4f4-90ea-428b-a6b1-aa9d73286ba4",
   "metadata": {},
   "source": [
    "## Unzip the quarterly ZIP file to a quarterly directory within the data directory  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2096b3-c757-4e19-843c-d2e7072683e0",
   "metadata": {},
   "source": [
    "A function is defined and used to uncompress the ZIP file and extract the CSV files to a quarterly directory for the first quarter of 2022.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f4630e-cc81-4cb7-ae52-73ea3161ab07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:19:24.088132Z",
     "iopub.status.busy": "2022-10-22T02:19:24.087921Z",
     "iopub.status.idle": "2022-10-22T02:19:24.095508Z",
     "shell.execute_reply": "2022-10-22T02:19:24.095048Z",
     "shell.execute_reply.started": "2022-10-22T02:19:24.088113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# https://realpython.com/working-with-files-in-python/\n",
    "# https://docs.python.org/3/library/zipfile.html\n",
    "# https://docs.python.org/3/library/re.html\n",
    "# https://regex101.com/\n",
    "# https://stackoverflow.com/questions/6773584/how-are-glob-globs-return-values-ordered\n",
    "# https://docs.python.org/3/library/pathlib.html\n",
    "# https://docs.python.org/3/library/shutil.html\n",
    "# https://stackoverflow.com/questions/31813504/move-and-replace-if-same-file-name-already-exists\n",
    "\n",
    "def unzip_file(zip_file_path, zip_file):\n",
    "    \"\"\"\n",
    "    Unzip a ZIP file from a path where the ZIP files\n",
    "    exist to expose the CSV files. If the CSV file matches a \n",
    "    regular expression, move the CSV file to the \n",
    "    '/data/q#_202#' directory of the current working \n",
    "    directory.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    zip_file_path -- the ZIP file path of the zipped file\n",
    "    zip_file -- the ZIP file name to be unzipped\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the 'with' statemeent, the ZIP file ('zip_file') is read \n",
    "    # and a ZipFile object ('zip_file_object') is created.\n",
    "    with zipfile.ZipFile(zip_file_path.joinpath(zip_file)) \\\n",
    "    as zip_file_object:\n",
    "        \n",
    "        # Try to make the 'temp' directory joined to 'zip_file_path' \n",
    "        # path, but if the directory already exists then pass.\n",
    "        try:\n",
    "            os.makedirs(zip_file_path.joinpath('temp'), \n",
    "                        exist_ok=False)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        # Try to make the sliced 'zip_file' directory joined to \n",
    "        # 'zip_file_path' path, but if the directory already exists \n",
    "        # then pass.\n",
    "        try:\n",
    "            os.makedirs(zip_file_path.joinpath((zip_file[5:12]).lower()), \n",
    "                        exist_ok=False)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        # The 'temp' directory is joined to the 'zip_file_path' path \n",
    "        # and assigned to the 'temp_path' variable.\n",
    "        temp_path = zip_file_path.joinpath('temp')\n",
    "        \n",
    "        # The sliced 'zip_file' directory is joined to the \n",
    "        # 'zip_file_path' path and assigned to the 'qtr_path' variable.\n",
    "        qtr_path =  zip_file_path.joinpath((zip_file[5:12].lower()))\n",
    "        \n",
    "        # Extract all folders and files from the ZipFile object \n",
    "        # ('zip_file_object') to the 'temp' directory ('temp_path').\n",
    "        zip_file_object.extractall(path=temp_path)\n",
    "        \n",
    "        # Assign 'csv_re_pattern' variable using \n",
    "        # regular expressions pattern to \n",
    "        # match '20yy-mm-dd.csv' format.\n",
    "        csv_re_pattern = '20(\\d{2})[/.-](\\d{2})[/.-](\\d{2}).csv$'\n",
    "        \n",
    "        # A 'for' loop iterates over the 'csv_file' CSV files in  \n",
    "        # the 'temp_path' directory and if the 'csv_file' matches \n",
    "        # the 'csv_re_pattern' regular expression pattern,\n",
    "        # the 'csv_file' is moved from the 'temp_path' directory to\n",
    "        # the 'qtr_path' directory.\n",
    "        # If the 'csv_file' CSV file already exists in \n",
    "        # the 'qtr_path' directorty, the file is overwritten. \n",
    "        # In the event, there is an exception, an error is raised\n",
    "        # and printed.\n",
    "        for csv_file in (csv_file for csv_file \\\n",
    "                         in sorted(temp_path.rglob('*.csv')) \\\n",
    "                         if re.match(csv_re_pattern, csv_file.name)):\n",
    "            src = str(csv_file)\n",
    "            dst = str(qtr_path) + '/' + str(csv_file.name)\n",
    "            try:\n",
    "                shutil.move(src, dst)\n",
    "            except BaseException as err:\n",
    "                print(f\"Error: {err}\")\n",
    "                \n",
    "        # Removes the 'zip_file' file\n",
    "        zip_file_path.joinpath(zip_file).unlink()\n",
    "        \n",
    "    # Removes the 'temp_path' directory and contents\n",
    "    # Since files and directories exist in the 'temp_path' directory, \n",
    "    # the 'shutil.rmtree' function is used to remove the whole \n",
    "    # directory tree\n",
    "    shutil.rmtree(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776579cc-0716-4699-bf6a-9f904e56442d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:19:24.097132Z",
     "iopub.status.busy": "2022-10-22T02:19:24.096899Z",
     "iopub.status.idle": "2022-10-22T02:19:50.591930Z",
     "shell.execute_reply": "2022-10-22T02:19:50.591384Z",
     "shell.execute_reply.started": "2022-10-22T02:19:24.097111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The path ('data_path') where the downloaded file ('dl_file') exist\n",
    "# and the downloaded file ('dl_file') are passed to the 'unzip_file'\n",
    "# function.\n",
    "unzip_file(data_path, dl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54b979-39b0-4f8b-a171-5e6736750654",
   "metadata": {},
   "source": [
    "## Convert the quarterly grouped CSV files into a Parquet file format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee10659-3eb0-4836-bac4-9c3956d46496",
   "metadata": {},
   "source": [
    "In an effort to improve performance, a decision was made to convert the data from the CSV file format to the Parquet file format. CSV files have the benefits of being a common file format accepted by many applications, being a human-readable format, and typically fast to write (Staubli, 2017). However, CSV files are not as efficient for complex data processing (Spicer, 2017). Due to the columnar storage format design, Parquet files are optimized for data analytical purposes (Staubli, 2017). Parquet files include a schema which reduces the computational of expense of infering a schema from a CSV file. In one comparison, the query time on the Parquet files was 34 times faster than CSV files (Yowakim, 2021). Another benefit of the Parquet file format is a reduced file size (Spicer, 2017).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9481fa-d5aa-4d33-a3b1-47e85c12637b",
   "metadata": {},
   "source": [
    "A function is defined and used to read and write the CSV files into a Parquet file for the first quarter of 2022.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74137044-9770-45b0-86e3-66176e5946c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:19:50.593032Z",
     "iopub.status.busy": "2022-10-22T02:19:50.592756Z",
     "iopub.status.idle": "2022-10-22T02:19:50.599726Z",
     "shell.execute_reply": "2022-10-22T02:19:50.599288Z",
     "shell.execute_reply.started": "2022-10-22T02:19:50.593012Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# https://arrow.apache.org/docs/python/dataset.html\n",
    "# https://stackoverflow.com/questions/71533197/how-do-i-specify-a-dtype-for-all-columns-when-reading-a-csv-file-with-pyarrow\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.dataset.CsvFileFormat.html\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.dataset.write_dataset.html\n",
    "# https://stackoverflow.com/questions/70478984/how-to-correct-csv-file-mixed-types-if-using-pyarrow-write-dataset-to-parquet\n",
    "# https://stackoverflow.com/questions/67071323/how-to-control-whether-pyarrow-dataset-write-dataset-will-overwrite-previous-dat\n",
    "\n",
    "def csv_to_parquet(csv_file_path):\n",
    "    \"\"\"\n",
    "    Accepts a path to a location of CSV files. Reads the CSV files into\n",
    "    a PyArrow Dataset and interprets the schema. The fields matching \n",
    "    'smart' are assigned to an int64 data type and the schema is \n",
    "    updated. The PyArrow Dataset is written to Parquet file.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    csv_file_path --  the path location of the CSV files\n",
    "    \"\"\"\n",
    "    # Try to make 'parquet' directory joined to 'data_path' path, \n",
    "    # but if the directory already exists then pass.\n",
    "    try:\n",
    "        os.makedirs(data_path.joinpath('parquet'), \n",
    "                    exist_ok=False)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    # The 'parquet' directory is joined to the 'data_path' and assigned \n",
    "    # to the 'pq_path' variable.\n",
    "    pq_path = data_path.joinpath('parquet')\n",
    "    \n",
    "    # To create a unique Parquet file name, the 'csv_file_path' string\n",
    "    # is sliced and appended with '{i}' for the iterator and '.parquet'\n",
    "    # file suffix.\n",
    "    pq_name_string = str(csv_file_path)[-7:] + '-' + '{i}' + '.parquet'\n",
    "    \n",
    "    # The 'dataset' function from PyArrow 'dataset' module scans\n",
    "    # the 'csv_file_path' directory for all the CSV files\n",
    "    # A 'dataset' object is created and assigned to the 'dataset' \n",
    "    # variable.\n",
    "    # The schema is inferred from the first CSV file which has missing\n",
    "    # values and results, resulting in 'fields' having 'null' data \n",
    "    # type.  \n",
    "    dataset = ds.dataset(csv_file_path, format='csv')\n",
    "    \n",
    "    # Create 'column_types' empty dictionary to store the column names \n",
    "    # andcolumn types from the 'dataset' schema.\n",
    "    column_types = {}\n",
    "    \n",
    "    # Assign 'field_re_pattern' variable using regular expressions \n",
    "    # pattern to match 'smart*' format.\n",
    "    field_smart_pattern = 'smart*'\n",
    "    \n",
    "    # A 'for' loop interates through each 'field' in the 'dataset' \n",
    "    # schema and if the 'field.name' matches the 'field_smart_pattern' \n",
    "    # regular expression, the 'field.name' is assigned PyArrow 'int64' \n",
    "    # data type and collected in the 'column_types' dictionary.\n",
    "    for field in dataset.schema:\n",
    "        if re.search(field_smart_pattern, field.name):\n",
    "            column_types[field.name] = pa.int64()\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # The 'column_types' dictionary with the 'field.names' and updated\n",
    "    # PyArrow 'int64' data types are passed to the 'column_types'\n",
    "    # argument of the 'ConvertOptions' class from PyArrow'csv' module.\n",
    "    convert_options = csv.ConvertOptions(column_types=column_types)\n",
    "    \n",
    "    # The 'convert_options' ConvertOptions object is passed to \n",
    "    # 'csv_file_format' CsvFileFormat object.\n",
    "    csv_file_format = ds.CsvFileFormat(convert_options=convert_options)\n",
    "    \n",
    "    # Again, the 'dataset' function from PyArrow 'dataset' module scans\n",
    "    # the 'csv_file_path' directory for all the CSV files.\n",
    "    # However, the 'csv_file_format' CsvFileFormat object is used to \n",
    "    # map the column names to the column types.\n",
    "    dataset = ds.dataset(csv_file_path, format=csv_file_format)\n",
    "    \n",
    "    # The 'write_dataset' fucntion from PyArrow 'dataset' module writes\n",
    "    # the 'dataset' object using a Parquet file format to the 'pq_path' \n",
    "    # directory and uses the 'pq_name_string' as the file name.\n",
    "    # In the event the file exists, the file will be overwritten.\n",
    "    ds.write_dataset(dataset, base_dir=pq_path,\n",
    "                     basename_template=pq_name_string,\n",
    "                     format='parquet',\n",
    "                     existing_data_behavior='overwrite_or_ignore')\n",
    "    \n",
    "    # Removes the 'csv_file_path' directory and contents.\n",
    "    # Since files and exist in the 'csv_file_path' directory, the\n",
    "    # 'shutil.rmtree' function is used to remove the whole directory \n",
    "    # tree.\n",
    "    shutil.rmtree(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26d6603-ebe0-4e95-9cd3-0a27c1b014fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:19:50.600479Z",
     "iopub.status.busy": "2022-10-22T02:19:50.600303Z",
     "iopub.status.idle": "2022-10-22T02:19:50.606496Z",
     "shell.execute_reply": "2022-10-22T02:19:50.605983Z",
     "shell.execute_reply.started": "2022-10-22T02:19:50.600463Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'q1_2022' path is joined to the 'data' path to create the \n",
    "# 'q1_2022_path' path as the location of quarter of CSV files.\n",
    "q1_2022_path = data_path.joinpath('q1_2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7905824e-ccbf-4feb-acb8-66c837c93e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:19:50.607234Z",
     "iopub.status.busy": "2022-10-22T02:19:50.607061Z",
     "iopub.status.idle": "2022-10-22T02:22:16.790740Z",
     "shell.execute_reply": "2022-10-22T02:22:16.790192Z",
     "shell.execute_reply.started": "2022-10-22T02:19:50.607217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'q1_2022_path' path is passed to the 'csv_to_parquet' function, \n",
    "# where the CSV files will be read and converted to a Parquet file.\n",
    "csv_to_parquet(q1_2022_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845debca-eefc-497f-94cb-3f51d65dfd64",
   "metadata": {},
   "source": [
    "## Inspect the Parquet file metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5bb61-f686-428b-b654-df85c23b5166",
   "metadata": {},
   "source": [
    "The metadata exposes how the Parquet file was created, as well as the number of columns, number of rows, number of row groups, format version, and serialized size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8197f6b-6585-4654-a814-b6bf92d98522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:22:16.792235Z",
     "iopub.status.busy": "2022-10-22T02:22:16.791848Z",
     "iopub.status.idle": "2022-10-22T02:22:16.795875Z",
     "shell.execute_reply": "2022-10-22T02:22:16.795251Z",
     "shell.execute_reply.started": "2022-10-22T02:22:16.792201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 'parquet' directory is assigned to the 'pq_path' variable.\n",
    "pq_path = data_path.joinpath('parquet')\n",
    "\n",
    "# The 'pq_files' list is established using a collection of Parquet \n",
    "# files to be examined.\n",
    "pq_file = 'q1_2022-0.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244ec137-8f52-47fd-b8c1-4ed6ef04c0d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:22:16.797142Z",
     "iopub.status.busy": "2022-10-22T02:22:16.796779Z",
     "iopub.status.idle": "2022-10-22T02:22:18.340658Z",
     "shell.execute_reply": "2022-10-22T02:22:18.339897Z",
     "shell.execute_reply.started": "2022-10-22T02:22:16.797111Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_2022-0.parquet\n",
      "<pyarrow._parquet.FileMetaData object at 0x7f20a13e7950>\n",
      "  created_by: parquet-cpp-arrow version 9.0.0\n",
      "  num_columns: 179\n",
      "  num_rows: 18845260\n",
      "  num_row_groups: 6366\n",
      "  format_version: 1.0\n",
      "  serialized_size: 123756203\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_metadata.html\n",
    "\n",
    "# The Parquet file ('pq_file') and Parquet file ('pq_file') is passed\n",
    "# to the PyArrow 'read_metadata' function to create 'pq_md'. The \n",
    "# Parquet file ('pq_file') and Parquet metadata ('pq_md') are \n",
    "# displayed.\n",
    "pq_md = pq.read_metadata(str(pq_path) + '/' + pq_file)\n",
    "print(pq_file)\n",
    "print(pq_md)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240a200-1511-4190-b323-da32a90947cb",
   "metadata": {},
   "source": [
    "The number of columns and rows provide some insight into the dimensions of the data. The Parquet file consists of 179 columns and 18,845,260 rows; a sizeable amount of data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd194a-c593-41c9-90da-c7cda91ad6dc",
   "metadata": {},
   "source": [
    "## Inspect the Parquet file schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a1958-d3d8-43b1-9eab-a544725f071c",
   "metadata": {},
   "source": [
    "The schema reveals the Parquet file field names and data types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38000c2e-d873-4e33-b8de-c10e9b7ff8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:22:18.342107Z",
     "iopub.status.busy": "2022-10-22T02:22:18.341728Z",
     "iopub.status.idle": "2022-10-22T02:22:19.893405Z",
     "shell.execute_reply": "2022-10-22T02:22:19.892713Z",
     "shell.execute_reply.started": "2022-10-22T02:22:18.342074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_2022-0.parquet\n",
      "date: date32[day]\n",
      "serial_number: string\n",
      "model: string\n",
      "capacity_bytes: int64\n",
      "failure: int64\n",
      "smart_1_normalized: int64\n",
      "smart_1_raw: int64\n",
      "smart_2_normalized: int64\n",
      "smart_2_raw: int64\n",
      "smart_3_normalized: int64\n",
      "smart_3_raw: int64\n",
      "smart_4_normalized: int64\n",
      "smart_4_raw: int64\n",
      "smart_5_normalized: int64\n",
      "smart_5_raw: int64\n",
      "smart_7_normalized: int64\n",
      "smart_7_raw: int64\n",
      "smart_8_normalized: int64\n",
      "smart_8_raw: int64\n",
      "smart_9_normalized: int64\n",
      "smart_9_raw: int64\n",
      "smart_10_normalized: int64\n",
      "smart_10_raw: int64\n",
      "smart_11_normalized: int64\n",
      "smart_11_raw: int64\n",
      "smart_12_normalized: int64\n",
      "smart_12_raw: int64\n",
      "smart_13_normalized: int64\n",
      "smart_13_raw: int64\n",
      "smart_15_normalized: int64\n",
      "smart_15_raw: int64\n",
      "smart_16_normalized: int64\n",
      "smart_16_raw: int64\n",
      "smart_17_normalized: int64\n",
      "smart_17_raw: int64\n",
      "smart_18_normalized: int64\n",
      "smart_18_raw: int64\n",
      "smart_22_normalized: int64\n",
      "smart_22_raw: int64\n",
      "smart_23_normalized: int64\n",
      "smart_23_raw: int64\n",
      "smart_24_normalized: int64\n",
      "smart_24_raw: int64\n",
      "smart_160_normalized: int64\n",
      "smart_160_raw: int64\n",
      "smart_161_normalized: int64\n",
      "smart_161_raw: int64\n",
      "smart_163_normalized: int64\n",
      "smart_163_raw: int64\n",
      "smart_164_normalized: int64\n",
      "smart_164_raw: int64\n",
      "smart_165_normalized: int64\n",
      "smart_165_raw: int64\n",
      "smart_166_normalized: int64\n",
      "smart_166_raw: int64\n",
      "smart_167_normalized: int64\n",
      "smart_167_raw: int64\n",
      "smart_168_normalized: int64\n",
      "smart_168_raw: int64\n",
      "smart_169_normalized: int64\n",
      "smart_169_raw: int64\n",
      "smart_170_normalized: int64\n",
      "smart_170_raw: int64\n",
      "smart_171_normalized: int64\n",
      "smart_171_raw: int64\n",
      "smart_172_normalized: int64\n",
      "smart_172_raw: int64\n",
      "smart_173_normalized: int64\n",
      "smart_173_raw: int64\n",
      "smart_174_normalized: int64\n",
      "smart_174_raw: int64\n",
      "smart_175_normalized: int64\n",
      "smart_175_raw: int64\n",
      "smart_176_normalized: int64\n",
      "smart_176_raw: int64\n",
      "smart_177_normalized: int64\n",
      "smart_177_raw: int64\n",
      "smart_178_normalized: int64\n",
      "smart_178_raw: int64\n",
      "smart_179_normalized: int64\n",
      "smart_179_raw: int64\n",
      "smart_180_normalized: int64\n",
      "smart_180_raw: int64\n",
      "smart_181_normalized: int64\n",
      "smart_181_raw: int64\n",
      "smart_182_normalized: int64\n",
      "smart_182_raw: int64\n",
      "smart_183_normalized: int64\n",
      "smart_183_raw: int64\n",
      "smart_184_normalized: int64\n",
      "smart_184_raw: int64\n",
      "smart_187_normalized: int64\n",
      "smart_187_raw: int64\n",
      "smart_188_normalized: int64\n",
      "smart_188_raw: int64\n",
      "smart_189_normalized: int64\n",
      "smart_189_raw: int64\n",
      "smart_190_normalized: int64\n",
      "smart_190_raw: int64\n",
      "smart_191_normalized: int64\n",
      "smart_191_raw: int64\n",
      "smart_192_normalized: int64\n",
      "smart_192_raw: int64\n",
      "smart_193_normalized: int64\n",
      "smart_193_raw: int64\n",
      "smart_194_normalized: int64\n",
      "smart_194_raw: int64\n",
      "smart_195_normalized: int64\n",
      "smart_195_raw: int64\n",
      "smart_196_normalized: int64\n",
      "smart_196_raw: int64\n",
      "smart_197_normalized: int64\n",
      "smart_197_raw: int64\n",
      "smart_198_normalized: int64\n",
      "smart_198_raw: int64\n",
      "smart_199_normalized: int64\n",
      "smart_199_raw: int64\n",
      "smart_200_normalized: int64\n",
      "smart_200_raw: int64\n",
      "smart_201_normalized: int64\n",
      "smart_201_raw: int64\n",
      "smart_202_normalized: int64\n",
      "smart_202_raw: int64\n",
      "smart_206_normalized: int64\n",
      "smart_206_raw: int64\n",
      "smart_210_normalized: int64\n",
      "smart_210_raw: int64\n",
      "smart_218_normalized: int64\n",
      "smart_218_raw: int64\n",
      "smart_220_normalized: int64\n",
      "smart_220_raw: int64\n",
      "smart_222_normalized: int64\n",
      "smart_222_raw: int64\n",
      "smart_223_normalized: int64\n",
      "smart_223_raw: int64\n",
      "smart_224_normalized: int64\n",
      "smart_224_raw: int64\n",
      "smart_225_normalized: int64\n",
      "smart_225_raw: int64\n",
      "smart_226_normalized: int64\n",
      "smart_226_raw: int64\n",
      "smart_230_normalized: int64\n",
      "smart_230_raw: int64\n",
      "smart_231_normalized: int64\n",
      "smart_231_raw: int64\n",
      "smart_232_normalized: int64\n",
      "smart_232_raw: int64\n",
      "smart_233_normalized: int64\n",
      "smart_233_raw: int64\n",
      "smart_234_normalized: int64\n",
      "smart_234_raw: int64\n",
      "smart_235_normalized: int64\n",
      "smart_235_raw: int64\n",
      "smart_240_normalized: int64\n",
      "smart_240_raw: int64\n",
      "smart_241_normalized: int64\n",
      "smart_241_raw: int64\n",
      "smart_242_normalized: int64\n",
      "smart_242_raw: int64\n",
      "smart_244_normalized: int64\n",
      "smart_244_raw: int64\n",
      "smart_245_normalized: int64\n",
      "smart_245_raw: int64\n",
      "smart_246_normalized: int64\n",
      "smart_246_raw: int64\n",
      "smart_247_normalized: int64\n",
      "smart_247_raw: int64\n",
      "smart_248_normalized: int64\n",
      "smart_248_raw: int64\n",
      "smart_250_normalized: int64\n",
      "smart_250_raw: int64\n",
      "smart_251_normalized: int64\n",
      "smart_251_raw: int64\n",
      "smart_252_normalized: int64\n",
      "smart_252_raw: int64\n",
      "smart_254_normalized: int64\n",
      "smart_254_raw: int64\n",
      "smart_255_normalized: int64\n",
      "smart_255_raw: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_schema.html\n",
    "\n",
    "# The Parquet file ('pq_file') and Parquet file ('pq_file') is passed\n",
    "# to the PyArrow 'read_schema' function to create 'pq_rs'. The \n",
    "# Parquet file ('pq_file') and Parquet schema ('pq_rq') are displayed.\n",
    "pq_rs = pq.read_schema(str(pq_path) + '/' + pq_file)\n",
    "print(pq_file)\n",
    "print(pq_rs)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0c1d5-4d9d-47ad-8b77-7adf77af3f8d",
   "metadata": {},
   "source": [
    "The field names and data types appear to be correct and there are no fields that indicate a null value for the data type.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f423d3-519e-459a-b855-cfd13d7368ed",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e2d5c-e927-4dc9-95ff-98c003d268cc",
   "metadata": {},
   "source": [
    "The hard drive snapshots in CSV files for the first quarter of 2022 were downloaded in a ZIP file from the Backblaze website. The CSV files were extracted from the ZIP file to a quarterly directory. Using PyArrow, the CSV files were read and written to a Parquet file for improved performance. Lastly, the metadata and schema of the Parquet file were examined to understand the structure of the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2dd68-2ca1-4b82-90df-832cedb13e18",
   "metadata": {},
   "source": [
    "## To prevent memory issues with other notebooks, please shutdown the kernel to free up memory or uncomment the cell below and run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09858688-b710-43c7-8cc9-a58d999437e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-22T02:22:19.894799Z",
     "iopub.status.busy": "2022-10-22T02:22:19.894417Z",
     "iopub.status.idle": "2022-10-22T02:22:19.898125Z",
     "shell.execute_reply": "2022-10-22T02:22:19.897512Z",
     "shell.execute_reply.started": "2022-10-22T02:22:19.894765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0f7c4-481f-4ef7-9739-e2f1039fe54f",
   "metadata": {},
   "source": [
    "## <center>References</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f691b-0139-4496-b09e-add0a81e1ac3",
   "metadata": {},
   "source": [
    "Backblaze. (2022). <i>Hard drive data and stats.</i> https://www.backblaze.com/b2/hard-drive-test-data.html  \n",
    "<br>\n",
    "Beach, B. (2015, February 4). <i>Reliability data set for 41,000 hard drives now open-source.</i> Backblaze Blog. https://www.backblaze.com/blog/hard-drive-data-feb2015/  \n",
    "<br>\n",
    "F-score. (2022, September 18). In <i>Wikipedia</i>. https://en.wikipedia.org/w/index.php?title=F-score&oldid=1110996374  \n",
    "<br>\n",
    "Hewlett Packard Enterprise Development LP. (2022). <i>What are data center tiers?</i> https://www.hpe.com/uk/en/what-is/data-center-tiers.html  \n",
    "<br>\n",
    "Maintenance (technical). (2022, September 18). In <i>Wikipedia</i>. https://en.wikipedia.org/w/index.php?title=Maintenance_(technical)&oldid=1110994458  \n",
    "<br>\n",
    "Market Research Future. (2022, June 16). <i>Predictive maintenance market to hit USD 111.34 billion by 2030, at a CAGR of 26.2% - report by Market Research Future (MRFR).</i> GlobeNewswire. https://www.globenewswire.com/en/news-release/2022/06/16/2463729/0/en/Predictive-Maintenance-Market-to-Hit-USD-111-34-Billion-by-2030-at-a-CAGR-of-26-2-Report-by-Market-Research-Future-MRFR.html  \n",
    "<br>\n",
    "Predictive maintenance. (2022, September 21). In <i>Wikipedia</i>. https://en.wikipedia.org/w/index.php?title=Predictive_maintenance&oldid=1111479330  \n",
    "<br>\n",
    "S.M.A.R.T. (2022, September 9). In <i>Wikipedia</i>. https://en.wikipedia.org/w/index.php?title=S.M.A.R.T.&oldid=1109378579  \n",
    "<br>\n",
    "Spicer, T. (2017, June 14). <i>Apache Parquet: How to be a hero with the open-source columnar data format.</i> Openbridge. https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04  \n",
    "<br>\n",
    "Staubli, G. (2017, October 9). <i>Spark file format showdown – CSV vs JSON vs Parquet</i>. LinkedIn. https://www.linkedin.com/pulse/spark-file-format-showdown-csv-vs-json-parquet-garren-staubli/  \n",
    "<br>\n",
    "Tyrrell, J. (2022, July 28). <i>Efficiency gains: predictive maintenance supports data center operations.</i> TechHQ . https://techhq.com/2022/07/machine-learning-data-center-maintenance/  \n",
    "<br>\n",
    "Wilson, B. (2018, July 17). <i>Backblaze durability calculates at 99.999999999% — and why it doesn’t matter.</i> Backblaze. https://www.backblaze.com/blog/cloud-storage-durability/  \n",
    "<br>\n",
    "Yowakim, E. (2021, December 7). <i>Difference between Parquet and CSV.</i> LinkedIn. https://www.linkedin.com/pulse/difference-between-parquet-csv-emad-yowakim/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
